{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0704d29",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning for Text Classification\n",
    "This notebook demonstrates **parameter-efficient fine-tuning** using the Hugging Face `peft` library. We'll fine-tune a pre-trained transformer model on a financial sentiment classification task with minimal resources, leveraging techniques like **LoRA** (Low-Rank Adaptation).\n",
    "\n",
    "The overall pipeline includes:\n",
    "- Loading a text classification dataset\n",
    "- Tokenizing and preparing the data\n",
    "- Fine-tuning a model using PEFT\n",
    "- Comparing performance before and after tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "The data and the tech stack use in this fine-tuning project:\n",
    "* PEFT technique: Lora\n",
    "* Model: gpt2\n",
    "* Evaluation approach: Trainer\n",
    "* Fine-tuning dataset: zeroshot/twitter-financial-news-sentiment (https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aa84a",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "First, we ensure that required dependencies such as `datasets`, `transformers`, and `scikit-learn` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install the required version of datasets in case you have an older version\n",
    "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
    "# ! pip install -q \"datasets==2.15.0\"\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717876a8",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "We import all necessary libraries for model loading, data preprocessing, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be21450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "        TrainingArguments, Trainer, EvalPrediction, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8151b",
   "metadata": {},
   "source": [
    "## Step 3: Load and Explore the Dataset\n",
    "We'll use the `zeroshot/twitter-financial-news-sentiment` dataset available from Hugging Face. After loading it, we perform a train-test split and inspect the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.39k/1.39k [00:00<00:00, 1.38MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/859k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 859k/859k [00:00<00:00, 6.82MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:00<00:00,  6.97it/s]\n",
      "Downloading data: 100%|██████████| 217k/217k [00:00<00:00, 2.98MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00,  8.52it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1159.29it/s]\n",
      "Generating train split: 9543 examples [00:00, 246225.94 examples/s]\n",
      "Generating validation split: 2388 examples [00:00, 237576.74 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Fantasia : FURTHER INFORMATION IN RELATION TO THE CO-OPERATION AGREEMENT WITH SHENGYUAN  #Stock #MarketScreener… https://t.co/NkBhcbaRvs',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "dataset = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "\n",
    "# Perform the train-test split on the 'train' dataset with shuffling\n",
    "split_result = dataset['train'].train_test_split(test_size=0.3, shuffle=True, seed=88)\n",
    "\n",
    "# Update the dataset dictionary directly with the new splits\n",
    "dataset.update({\n",
    "    'train': split_result['train'],\n",
    "    'test': split_result['test']\n",
    "})\n",
    "\n",
    "# Showing first example for train set\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42df39f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 6680\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2388\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2863\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset stucture\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4be0a",
   "metadata": {},
   "source": [
    "## Step 4: Tokenization\n",
    "Here we initialize the tokenizer from our pre-trained model (e.g., GPT-2) and tokenize the text for input to the model. Tokenization ensures the model understands input sequences in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b226a8",
   "metadata": {},
   "source": [
    "The dataset contains training, test and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6354f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 118kB/s]\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 3.01MB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 11.9MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 6.62MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 25.8MB/s]\n",
      "Map: 100%|██████████| 6680/6680 [00:04<00:00, 1519.69 examples/s]\n",
      "Map: 100%|██████████| 2863/2863 [00:01<00:00, 1564.21 examples/s]\n",
      "Map: 100%|██████████| 2388/2388 [00:01<00:00, 1485.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6680\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2863\n",
      "}), 'validation': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2388\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Defining a function to tokenize a batch of texts\n",
    "def tokenize_batch(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize all the examples in each split of the dataset\n",
    "tokenized_dataset = {}\n",
    "splits = ['train', 'test', 'validation']\n",
    "for split in splits:\n",
    "    # Check if the split exists in the dataset to avoid KeyError\n",
    "    if split in dataset:\n",
    "        tokenized_dataset[split] = dataset[split].map(tokenize_batch, batched=True)\n",
    "\n",
    "# Show the structure of the tokenized dataset\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5428d5",
   "metadata": {},
   "source": [
    "## Step 5: Load and Prepare the Base Model\n",
    "We load a pre-trained GPT-2 model for sequence classification and adapt it for our task. Since we're doing parameter-efficient fine-tuning, we **freeze the base model's parameters** to train only a few layers or components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model with specific configuration options\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=3,\n",
    "    id2label={0: 'Negative', 1: 'Positive', 2: 'Indifferent'},\n",
    "    label2id={'Negative': 0, 'Positive': 1, 'Indifferent': 2}\n",
    ")\n",
    "\n",
    "# Update the model's tokenizer pad token id in its configuration\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8c4ba",
   "metadata": {},
   "source": [
    "## Step 6: Training the Base Model (Frozen)\n",
    "We define training arguments and use Hugging Face’s `Trainer` to fine-tune the model. Evaluation and saving are done after each epoch to monitor improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 548M/548M [00:02<00:00, 209MB/s]  \n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='836' max='836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [836/836 28:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.932271</td>\n",
       "      <td>0.648241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.747100</td>\n",
       "      <td>0.887464</td>\n",
       "      <td>0.652848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=836, training_loss=1.4194727628424977, metrics={'train_runtime': 1690.9183, 'train_samples_per_second': 7.901, 'train_steps_per_second': 0.494, 'total_flos': 6981912216207360.0, 'train_loss': 1.4194727628424977, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/train\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 16,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049cb91",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation on the Test Set\n",
    "After training, we evaluate the model on the test set to measure performance. We compute the **accuracy** and display a simple comparison of predictions vs. ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb84dd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8874643445014954,\n",
       " 'eval_accuracy': 0.6528475711892797,\n",
       " 'eval_runtime': 211.8086,\n",
       " 'eval_samples_per_second': 11.274,\n",
       " 'eval_steps_per_second': 0.708,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af42e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pre-trained model\n",
    "model.save_pretrained('gpt2-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e47b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the test set\n",
    "predicted = trainer.predict(tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5845356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.62%\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"predictions\": predicted.predictions.argmax(axis=1),\n",
    "        \"actual\": predicted.label_ids,\n",
    "    }\n",
    ")\n",
    "df\n",
    "\n",
    "accuracy = (df['predictions'] == df['actual']).mean()\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcff8e8",
   "metadata": {},
   "source": [
    "## Step 8: Introduce Parameter-Efficient Fine-Tuning (PEFT) with LoRA\n",
    "Now, we enhance our approach using **LoRA**—a PEFT technique that adds a small number of trainable parameters. This allows effective fine-tuning with drastically reduced compute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da94b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import  get_peft_config, get_peft_model,\\\n",
    "            LoraConfig,  TaskType, AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a6ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07b33ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 308,736 || all params: 124,748,544 || trainable%: 0.2474866560366428\n"
     ]
    }
   ],
   "source": [
    "# PEFT model configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=3,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.2,\n",
    "    bias = 'none',\n",
    "    target_modules=['c_attn', 'c_proj']\n",
    ")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('gpt2-model')\n",
    "\n",
    "# Create the lora model\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b228080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the trainer parameters\n",
    "training_arg = TrainingArguments(\n",
    "        output_dir=\"./data/train-lora\",\n",
    "        # Set the learning rate\n",
    "        learning_rate = 2e-3,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_arg,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e86ea2",
   "metadata": {},
   "source": [
    "## Step 9: Fine-Tune with LoRA\n",
    "We configure and train the LoRA-adapted model. The training process now updates only the small LoRA modules, while the main model remains frozen. This is ideal for low-resource environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe5d0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1670' max='1670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1670/1670 29:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.471124</td>\n",
       "      <td>0.846734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1670, training_loss=0.7094938906366953, metrics={'train_runtime': 1756.4349, 'train_samples_per_second': 3.803, 'train_steps_per_second': 0.951, 'total_flos': 3503532665733120.0, 'train_loss': 0.7094938906366953, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the PEFT model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55472edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='856' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [597/597 05:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.47112417221069336,\n",
       " 'eval_accuracy': 0.8467336683417085,\n",
       " 'eval_runtime': 233.9509,\n",
       " 'eval_samples_per_second': 10.207,\n",
       " 'eval_steps_per_second': 2.552,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b281ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Lora model\n",
    "lora_model.save_pretrained(\"lora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7338eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the test set using the pre-trained model\n",
    "predicted = trainer.predict(tokenized_dataset['test'])\n",
    "actual = np.array(tokenized_dataset['test']['label'])\n",
    "x = np.stack((predicted.label_ids, actual))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"predictions\": predicted.predictions.argmax(axis=1),\n",
    "        \"actual\": predicted.label_ids,\n",
    "    }\n",
    ")\n",
    "df\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (df['predictions'] == df['actual']).mean()\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3a1f0",
   "metadata": {},
   "source": [
    "The accuracy of the fine-tuned model 83.37% is higher than the pre-trained model 64.62%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Step 10: Performing Inference with a PEFT Model\n",
    "\n",
    "Now we load the saved PEFT model weights and take a few samples to evaluate the performance of the trained PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056d12bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=3, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=3, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Linear(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=3, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=3, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=3, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=3, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the model\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\"lora-model\")\n",
    "inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98b783de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. It's Official: Nio Brings Former Auto Analyst Wei Feng On As CFO\n",
      "Sentiment: Indifferent\n",
      "\n",
      "2. Lumber Liquidators +1.3% after guidance update\n",
      "Sentiment: Positive\n",
      "\n",
      "3. Suzuki considers China supply options, third-quarter profit falls 11%\n",
      "Sentiment: Negative\n",
      "\n",
      "4. Teva, Bausch Could Be Next to File for Bankruptcy\n",
      "Sentiment: Negative\n",
      "\n",
      "5. $NLOK - Taking A Look At The Special Situation Opportunity In NortonLifeLock. https://t.co/wrZvz2jZFb #markets #economy #finance\n",
      "Sentiment: Indifferent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 tweets and sentiments in the test set\n",
    "n = 5\n",
    "tweets_list = (tokenized_dataset[\"test\"]['text'][:5], tokenized_dataset[\"test\"]['label'][:5])\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {tweets_list[0][i]}\")\n",
    "    print(f\"Sentiment: {inference_model.config.id2label[tweets_list[1][i]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Inference function\n",
    "def get_sentiment(tweet):\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\")\n",
    "    logits = inference_model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    return inference_model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bee290c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. It's Official: Nio Brings Former Auto Analyst Wei Feng On As CFO\n",
      "Sentiment: Indifferent\n",
      "\n",
      "2. Lumber Liquidators +1.3% after guidance update\n",
      "Sentiment: Positive\n",
      "\n",
      "3. Suzuki considers China supply options, third-quarter profit falls 11%\n",
      "Sentiment: Negative\n",
      "\n",
      "4. Teva, Bausch Could Be Next to File for Bankruptcy\n",
      "Sentiment: Indifferent\n",
      "\n",
      "5. $NLOK - Taking A Look At The Special Situation Opportunity In NortonLifeLock. https://t.co/wrZvz2jZFb #markets #economy #finance\n",
      "Sentiment: Indifferent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the fine-tuned model to perfrom inference\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. {tweets_list[0][i]}\")\n",
    "    print(f\"Sentiment: {get_sentiment(tweets_list[0][i])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b98ce",
   "metadata": {},
   "source": [
    "The fine-tuned model made correct predictions for 4 out of the 5 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996f4a4",
   "metadata": {},
   "source": [
    "## ✅ Conclusion & Key Takeaways\n",
    "In this project, we demonstrated how to apply **Lightweight Fine-Tuning** using Hugging Face and the `peft` library. By utilizing **LoRA (Low-Rank Adaptation)**, we significantly reduced the number of trainable parameters while still achieving meaningful performance.\n",
    "\n",
    "**Key Highlights:**\n",
    "- Used GPT-2 for a sequence classification task (financial sentiment analysis)\n",
    "- Fine-tuned the base model by freezing its core layers and training only classification heads\n",
    "- Applied LoRA for parameter-efficient fine-tuning, greatly reducing compute overhead\n",
    "- Achieved solid classification performance while maintaining resource efficiency\n",
    "\n",
    "This approach is particularly useful when working in resource-constrained environments or deploying models in production with limited infrastructure.\n",
    "\n",
    "🚀 *Next steps could involve experimenting with other PEFT methods, testing on different datasets, or deploying the model in an API-powered inference system.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
